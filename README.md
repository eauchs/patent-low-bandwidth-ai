# üöÄ HYBRID-ADVANCED

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE) ## üåü Pr√©sentation

HYBRID-ADVANCED est un projet combinant un serveur Flask pour un mod√®le de langage visuel (VLM) et une application Flask principale int√©grant diverses fonctionnalit√©s d'IA, telles que l'extraction de texte √† partir d'images et de PDFs via un VLM local (SmolDocling), la recherche de documents pertinents (RAG) avec ChromaDB, la r√©cup√©ration d'informations de Wikip√©dia et la gestion de l'historique des conversations, et la possibilit√© de converser par SMS avec le backend. La g√©n√©ration de texte finale est g√©r√©e par un serveur VLM d√©di√©.

Ce projet offre une architecture modulaire pour interagir avec des mod√®les de langage visuel, permettant des applications allant de la question-r√©ponse bas√©e sur des images et des documents √† des syst√®mes de dialogue plus complexes.

## üõ†Ô∏è Fonctionnalit√©s Cl√©s

* **Serveur de Mod√®le de Langage Visuel (VLM) :**
    * Chargement √† la demande de mod√®les VLM (actuellement configur√© pour Mistral Small).
    * G√©n√©ration de texte conditionn√©e par un prompt et une image (optionnelle).
    * API simple bas√©e sur Flask pour interagir avec le mod√®le.
* **Application Principale :**
    * **Extraction de Texte Avanc√©e :** Utilisation locale de SmolDocling pour extraire le texte d'images et de fichiers PDF.
    * **R√©cup√©ration Augment√©e par la G√©n√©ration (RAG) :** Recherche de documents pertinents dans une base de connaissances ChromaDB.
    * **Int√©gration Wikip√©dia :** R√©cup√©ration de contexte pertinent √† partir de Wikip√©dia.
    * **Gestion de l'Historique :** Suivi de l'historique des conversations via ChromaDB.
    * **G√©n√©ration de Texte Externe :** Utilisation d'un serveur Flask d√©di√© pour la g√©n√©ration de texte finale, offrant une s√©paration des responsabilit√©s et une flexibilit√© potentielle pour diff√©rents mod√®les de langage.
    * **API RESTful :** Exposition des fonctionnalit√©s via des API Flask claires et document√©es.

## üöÄ D√©marrage

Suivez ces √©tapes pour configurer et ex√©cuter le projet sur votre machine.

### Pr√©requis

Assurez-vous d'avoir les logiciels suivants install√©s :

* [Python 3.x](https://www.python.org/downloads/)
* [pip](https://pip.pypa.io/en/stable/installing/) (install√© avec Python)
* [Git](https://git-scm.com/downloads)

### Installation

1.  **Cloner le R√©pertoire :**

    ```bash
    git clone https://github.com/eauchs/hybrid-advanced.git
    cd hybrid-advanced
    ```

2.  **Cr√©er et Activer un Environnement Virtuel (Recommand√©) :**

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Sur Linux/macOS
    # venv\Scripts\activate  # Sur Windows
    ```

3.  **Installer les D√©pendances :**

    ```bash
    # Pour le serveur VLM (ex√©cuter dans le r√©pertoire contenant vlmserver.py)
    cd server
    pip install -r requirements.txt
    cd ..

    # Pour l'application principale (ex√©cuter dans le r√©pertoire racine du projet)
    cd hybrid-advanced
    pip install -r requirements.txt
    cd ..
    ```

4.  **Configuration :**

    * Cr√©ez un fichier `.env` dans le r√©pertoire racine de chaque application (serveur VLM et application principale) et configurez les variables d'environnement n√©cessaires. Voici un exemple de variables que vous pourriez avoir besoin de configurer :

        **Pour le serveur VLM (`server/.env`) :**

        ```env
        VLM_MODEL_PATH="mlx-community/Mistral-Small-3.1-24B-Instruct-2503-3bit"
        SERVER_HOST="0.0.0.0"
        SERVER_PORT=5001
        ```

        **Pour l'application principale (`main_app_hybrid/.env`) :**

        ```env
        TEMP_FOLDER="/tmp"
        CHROMA_DB_PATH="chromadb_base"
        CHROMA_COLLECTION_NAME="my_collection"
        CHROMA_SMS_HISTORY_COLLECTION_NAME="sms_history_collection"
        EMBEDDING_MODEL_NAME="ibm-granite/granite-embedding-278m-multilingual"
        RERANKER_MODEL_NAME="ibm-research/re2g-reranker-nq"
        SMOL_VLM_MODEL_PATH="ds4sd/SmolDocling-256M-preview-mlx-bf16" # Si vous utilisez SmolDocling localement
        TEXT_GENERATION_SERVER_ENDPOINT="http://localhost:5001/api/vlm_generate"
        REQUEST_TIMEOUT=1000
        SMOL_VLM_MAX_TOKENS=1024
        SMOL_VLM_TEMPERATURE=0.0
        WIKIPEDIA_LANG="fr"
        WIKIPEDIA_MAX_CHARS=8000
        RAG_N_RESULTS=15
        RAG_MIN_TOKEN_COUNT=10
        RAG_TOP_K_RERANKED=10
        CONTEXT_MAX_CHARS_DOCUMENT=6000
        CONTEXT_MAX_CHARS_RAG=10000
        CONTEXT_MAX_CHARS_WIKI=4000
        SMS_HISTORY_RAG_N_RESULTS=10
        CONTEXT_MAX_CHARS_SMS_HISTORY=5000
        PDF_MAX_PAGES=10
        PDF_DPI=200
        PDF_CONVERT_TIMEOUT=120
        ```

        **Note :** Ajustez les chemins et les valeurs en fonction de votre configuration.

### Ex√©cution

1.  **D√©marrer le Serveur VLM :**

    ```bash
    cd server
    python server.py
    ```

    Le serveur devrait d√©marrer sur `http://0.0.0.0:5001` (ou l'adresse et le port configur√©s).

2.  **D√©marrer l'Application Principale :**

    ```bash
    cd main_app_hybrid
    python main_app_hybrid.py
    ```

    L'application principale devrait d√©marrer sur `http://0.0.0.0:500` (par d√©faut).

## ‚öôÔ∏è Utilisation

D√©crivez ici comment interagir avec votre application. Par exemple :

* Pour interagir avec l'application principale, vous pouvez envoyer des requ√™tes POST √† l'endpoint `/api/generate` avec un payload JSON contenant l'historique des conversations, les donn√©es de fichier (optionnel) et les options de traitement (RAG, Wikip√©dia, document).
* Pour le serveur VLM, vous pouvez envoyer des requ√™tes POST √† `/api/vlm_generate` avec un prompt et une image base64 (optionnelle) pour obtenir une r√©ponse textuelle.

**Exemple de requ√™te √† l'application principale (`/api/generate`) :**

```json
{
  "conversations": [
    {
      "role": "user",
      "content": [
        { "type": "text", "text": "Quel est le contenu de cette image ?" }
      ]
    }
  ],
  "file_data": "...", // Votre image en base64
  "file_name": "image.png",
  "options": {
    "document": true,
    "rag": false,
    "wikipedia": false
  }
}

{
  "prompt": "D√©crivez cette image.",
  "image_base64": "...", // Votre image en base64 (optionnel)
  "max_tokens": 256,
  "temperature": 0.7
}

# hybrid-advanced
